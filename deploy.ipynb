{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map_s2i = {\n",
    "    'person': 0, 'car': 1, 'building': 2, 'window': 3, 'tree': 4,\n",
    "    'sign': 5, 'door': 6, 'bookshelf': 7, 'chair': 8, 'table': 9,\n",
    "    'keyboard': 10, 'head': 11, 'Unknown': 12\n",
    "}\n",
    "\n",
    "class_map_i2s = {v: k for k, v in class_map_s2i.items()}\n",
    "\n",
    "class_names = list(class_map_s2i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mobilenet_v3_small(model_path=\"models/pretrained_MobileNet_V3_small.pth\"):\n",
    "    # Create an EffNetB2 feature extractor\n",
    "    def create_mobilenet_v3_small():\n",
    "        # Set the manual seeds\n",
    "        torch.manual_seed(42)\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "        # Get the length of class_names (one output unit for each class)\n",
    "        class_names = class_map_s2i.keys()\n",
    "        output_shape = len(class_names)\n",
    "\n",
    "        # Get the base model with pretrained weights and send to target device\n",
    "        weights = torchvision.models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "        model = torchvision.models.mobilenet_v3_small(weights=weights).to('cpu')\n",
    "\n",
    "        # Change the classifier head\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=576, out_features=output_shape)\n",
    "        ).to('cpu')\n",
    "\n",
    "        # Give the model a name\n",
    "        model_name = \"mobilenet_v3_small\"\n",
    "        print(f\"[INFO] Created new {model_name} model.\")\n",
    "        return model\n",
    "\n",
    "    saved_model = create_mobilenet_v3_small().to('cpu')\n",
    "    saved_model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    saved_model.eval()\n",
    "    return saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new mobilenet_v3_small model.\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def predict_image(image, model=load_mobilenet_v3_small()):\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = timer()\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    #image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n",
    "        pred_probs = torch.softmax(model(image), dim=1)\n",
    "    \n",
    "    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n",
    "    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
    "    \n",
    "    # Calculate the prediction time\n",
    "    pred_time = round(timer() - start_time, 5)\n",
    "    \n",
    "    # Return the prediction dictionary and prediction time \n",
    "    return pred_labels_and_probs, pred_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load the image\n",
    "image = Image.open('chair.jpg')\n",
    "\n",
    "pred_labels_and_probs, pred_time = predict_image(image)\n",
    "print(pred_labels_and_probs)\n",
    "print(pred_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list = ['car2.jpg', 'person.jpg', 'chair.jpg', 'window.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Setting up a public link... we have recently upgraded the way public links are generated. If you encounter any problems, please report the issue and downgrade to gradio version 3.13.0\n",
      ".\n",
      "Running on public URL: https://ae0f1094-3159-41b1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ae0f1094-3159-41b1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Create title, description and article strings\n",
    "title = \"LabelMe Classifier \\n üôçüöóüè†ü™üüå≥üõëüö™ü™ë‚å®Ô∏è\"\n",
    "description = \"A MobileNetV3 feature extractor computer vision model to classify images trained on the LabelMe dataset.\"\n",
    "article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n",
    "\n",
    "# Create the Gradio demo\n",
    "demo = gr.Interface(fn=predict_image, # mapping function from input to output\n",
    "                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n",
    "                    outputs=[gr.Label(num_top_classes=1, label=\"ü§î...That Looks like a:\"), # what are the outputs?\n",
    "                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n",
    "                    examples=example_list, \n",
    "                    title=title,\n",
    "                    description=description,\n",
    "                    article=article)\n",
    "\n",
    "# Launch the demo!\n",
    "demo.launch(debug=False, # print errors locally?\n",
    "            share=True) # generate a publically shareable URL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
