{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* importing necessary libraries to read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* setting paths for dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting paths for train and test data\n",
    "train_dir = \"train\"\n",
    "train_annot_path = os.path.join(train_dir, \"annotation.txt\")\n",
    "\n",
    "test_dir = \"test\"\n",
    "test_annot_path = os.path.join(test_dir, \"annotation.txt\")\n",
    "\n",
    "# setting paths for classes.txt\n",
    "classes_txt_path = \"classes.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* getting class labels from annotation and class files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting class labels from \"classes.txt\"\n",
    "with open(classes_txt_path) as file:\n",
    "    class_map_s2i = {line.rstrip(): i for i, line in enumerate(file.readlines()) if line.rstrip()}\n",
    "\n",
    "# Add \"clutter\" to the class_map_s2i\n",
    "class_map_s2i[\"clutter\"] = len(class_map_s2i)\n",
    "\n",
    "class_map_i2s = {v: k for k, v in class_map_s2i.items()}\n",
    "\n",
    "# getting labels for train data from \"train/annotation.txt\"\n",
    "with open(train_annot_path) as file:\n",
    "    train_annots = [line.rstrip().split() for line in file.readlines()]\n",
    "train_img_path = [os.path.join(train_dir, \"0\" + train_annot[0][:3], train_annot[0] + \".jpg\") for train_annot in train_annots]\n",
    "\n",
    "# Modify this part to classify as \"clutter\" if all values are -1.00\n",
    "train_int_lbl = []\n",
    "for train_annot in train_annots:\n",
    "    values = np.array([float(x) for x in train_annot[1:]])\n",
    "    if np.all(values == -1.00):\n",
    "        train_int_lbl.append(class_map_s2i[\"clutter\"])  # Use the integer label for \"clutter\"\n",
    "    else:\n",
    "        train_int_lbl.append(np.argmax(values))\n",
    "\n",
    "# Create train_str_lbl based on train_int_lbl\n",
    "train_str_lbl = [class_map_i2s[label] for label in train_int_lbl]\n",
    "\n",
    "# getting labels for test data from \"test/annotation.txt\"\n",
    "with open(test_annot_path) as file:\n",
    "    test_annots = [line.rstrip().split() for line in file.readlines()]\n",
    "test_img_path = [os.path.join(test_dir, \"0\" + test_annot[0][:3], test_annot[0] + \".jpg\") for test_annot in test_annots]\n",
    "\n",
    "# Modify this part to classify as \"clutter\" if all values are -1.00\n",
    "test_int_lbl = []\n",
    "for test_annot in test_annots:\n",
    "    values = np.array([float(x) for x in test_annot[1:]])\n",
    "    if np.all(values == -1.00):\n",
    "        test_int_lbl.append(class_map_s2i[\"clutter\"])  # Use the integer label for \"clutter\"\n",
    "    else:\n",
    "        test_int_lbl.append(np.argmax(values))\n",
    "\n",
    "# Create test_str_lbl based on test_int_lbl\n",
    "test_str_lbl = [class_map_i2s[label] for label in test_int_lbl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creating dataframes for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes for train and test data\n",
    "train_df = pd.DataFrame({\"img_path\":train_img_path, \"int_label\":train_int_lbl, \"str_label\":train_str_lbl})\n",
    "test_df = pd.DataFrame({\"img_path\":test_img_path, \"int_label\":test_int_lbl, \"str_label\":test_str_lbl})\n",
    "\n",
    "# displaying dataframes\n",
    "print(\"\\n\\nTRAIN DF\")\n",
    "display(train_df)\n",
    "print(train_df.str_label.value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n\\n\\nTEST DF\")\n",
    "display(test_df)\n",
    "print(test_df.str_label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* visualizing the images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying first image from train data\n",
    "print(f\"Image Class: {train_df.str_label[0]}\")\n",
    "print(f\"Image Height: {Image.open(train_df.img_path[0]).height}\")\n",
    "print(f\"Image Width: {Image.open(train_df.img_path[0]).width}\")\n",
    "Image.open(train_df.img_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying last image from train data\n",
    "print(f\"Image Class: {train_df.str_label[len(train_df)-1]}\")\n",
    "print(f\"Image Height: {Image.open(train_df.img_path[len(train_df)-1]).height}\")\n",
    "print(f\"Image Width: {Image.open(train_df.img_path[len(train_df)-1]).width}\")\n",
    "Image.open(train_df.img_path[len(train_df)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying first image from test data\n",
    "print(f\"Image Class: {test_df.str_label[0]}\")\n",
    "print(f\"Image Height: {Image.open(test_df.img_path[0]).height}\")\n",
    "print(f\"Image Width: {Image.open(test_df.img_path[0]).width}\")\n",
    "Image.open(test_df.img_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying 10 random images from train and test data each with their class labels\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 4))\n",
    "for i in range(10):\n",
    "    rand_int = random.randint(0, len(train_df)-1)\n",
    "    ax[0, i].imshow(Image.open(train_df.img_path[rand_int]))\n",
    "    ax[0, i].set_title(train_df.str_label[rand_int])\n",
    "    ax[0, i].axis(\"off\")\n",
    "\n",
    "    rand_int = random.randint(0, len(test_df)-1)\n",
    "    ax[1, i].imshow(Image.open(test_df.img_path[rand_int]))\n",
    "    ax[1, i].set_title(test_df.str_label[rand_int])\n",
    "    ax[1, i].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting class distribution for train data\n",
    "train_df['str_label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Creating Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* importing necessary libraries for dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* defining class to load custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx, 0]\n",
    "        img = Image.open(img_path)\n",
    "        label = int(self.dataframe.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* defining transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to the desired size\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])  # Normalized using ImageNet stats\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiating custom dataset for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, transform=transform)\n",
    "test_dataset = CustomDataset(test_df, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creating dataloaders for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of samples/batches per dataloader\n",
    "print(f\"Number of batches of size {batch_size} in training data: {len(train_loader)}\")\n",
    "print(f\"Number of batches of size {batch_size} in testing data: {len(test_loader)}\")\n",
    "print(f\"Number of classes: {len(class_map_s2i)}, class names: {class_map_s2i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Setting up device agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Defining Models that we are going to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FEATURES = len(class_map_s2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of EfficientNet-B0\n",
    "def create_effnet_b0():\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # set the seeds for reproducibility\n",
    "    set_seeds()\n",
    "\n",
    "    # change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"EfficientNet-B0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# create an instance of EfficientNet-B1\n",
    "def create_effnet_b1():\n",
    "    weights = torchvision.models.EfficientNet_B1_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b1(weights=weights).to(device)\n",
    "\n",
    "    # set the seeds for reproducibility\n",
    "    set_seeds()\n",
    "\n",
    "    # change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"EfficientNet-B1\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# create an instance of MobileNet-V2\n",
    "def create_mobilenet_v2():\n",
    "    weights = torchvision.models.MobileNetV2_Weights.DEFAULT\n",
    "    model = torchvision.models.mobilenet_v2(weights=weights).to(device)\n",
    "\n",
    "    # set the seeds for reproducibility\n",
    "    set_seeds()\n",
    "\n",
    "    # change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"MobileNet-V2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# create an instance of MobileNet-V3 small\n",
    "def create_mobilenet_v3_small():\n",
    "    weights = torchvision.models.MobileNetV3_SMALL_Weights.DEFAULT\n",
    "    model = torchvision.models.mobilenet_v3_small(weights=weights).to(device)\n",
    "\n",
    "    # set the seeds for reproducibility\n",
    "    set_seeds()\n",
    "\n",
    "    # change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=576, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"MobileNet-V3-small\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# create an instance of MobileNet-V3 LARGE\n",
    "def create_mobilenet_v3_large():\n",
    "    weights = torchvision.models.MobileNetV3_LARGE_Weights.DEFAULT\n",
    "    model = torchvision.models.mobilenet_v3_large(weights=weights).to(device)\n",
    "\n",
    "    # set the seeds for reproducibility\n",
    "    set_seeds()\n",
    "\n",
    "    # change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=960, out_features=OUT_FEATURES, bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"MobileNet-V3-large\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models = [\"EfficientNet-B0\", \"EfficientNet-B1\", \"MobileNet-V2\", \"MobileNet-V3-small\", \"MobileNet-V3-large\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import engine\n",
    "from pathlib import Path\n",
    "from utils import save_model\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each number of epochs\n",
    "for epochs in num_epochs:\n",
    "\n",
    "    # 5. Loop through each model name and create a new model based on the name\n",
    "    for model_name in models:\n",
    "\n",
    "        # 6. Create information print outs\n",
    "        experiment_number += 1\n",
    "        print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        print(f\"[INFO] Number of epochs: {epochs}\")  \n",
    "\n",
    "        # 7. Select the model\n",
    "        if model_name == \"EfficientNet-B0\":\n",
    "            model = create_effnet_b0() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "\n",
    "        elif model_name == \"EfficientNet-B1\":\n",
    "            model = create_effnet_b1() \n",
    "\n",
    "        elif model_name == \"MobileNet-V2\":\n",
    "            model = create_mobilenet_v2()\n",
    "\n",
    "        elif model_name == \"MobileNet-V3-small\":\n",
    "            model = create_mobilenet_v3_small()\n",
    "        else:\n",
    "            model = create_mobilenet_v3_large()\n",
    "            \n",
    "        # 8. Create a new loss and optimizer for every model\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "        # 9. Train target model with target dataloaders and track experiments\n",
    "        results = engine.train(model=model,\n",
    "                               train_dataloader=train_loader,\n",
    "                               test_dataloader=test_loader, \n",
    "                               optimizer=optimizer,\n",
    "                               loss_fn=loss_fn,\n",
    "                               epochs=epochs,\n",
    "                               device=device)\n",
    "\n",
    "        # 10. Save the model\n",
    "        save_filepath = f\"{model_name}_{epochs}_epochs.pth\"\n",
    "        save_model(model=model,\n",
    "                   target_dir=\"models\",\n",
    "                   model_name=save_filepath)\n",
    "\n",
    "\n",
    "        # save model accuracy and size\n",
    "        model_accuracy = results[\"test_acc\"][-1]\n",
    "        model_size = Path(f\"models/{save_filepath}\").stat().st_size // (1024 * 1024)\n",
    "\n",
    "        # Create a dictionary to store the values\n",
    "        results_dict = {\n",
    "            f\"{model_name}_{epochs}_accuracy\": model_accuracy,\n",
    "            f\"{model_name}_{epochs}_size\": model_size\n",
    "        }\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Trained Model Size: {results_dict[f'{model_name}_{epochs}_size']} MB | Trained Model Accuracy: {results_dict[f'{model_name}_{epochs}_accuracy'] * 100:.2f}%\")\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
    "summary(model, \n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getting class labels from \"classes.txt\"\n",
    "with open(classes_txt_path) as file:\n",
    "    class_map_s2i = {line.rstrip():i for i, line in enumerate(file.readlines()) if line.rstrip()}\n",
    "class_map_i2s = {v:k for k,v in class_map_s2i.items()}\n",
    "\n",
    "# getting labels for train data from \"train/annotation.txt\"\n",
    "with open(train_annot_path) as file:\n",
    "    train_annots = [line.rstrip().split() for line in file.readlines()]\n",
    "train_img_path = [os.path.join(train_dir, \"0\"+train_annot[0][:3], train_annot[0]+\".jpg\") for train_annot in train_annots]\n",
    "train_int_lbl = [np.argmax(np.array([int(ast.literal_eval(x)) for x in train_annot[1:]])) for train_annot in train_annots]\n",
    "train_str_lbl = [class_map_i2s[x] for x in train_int_lbl]\n",
    "\n",
    "# getting labels for test data from \"test/annotation.txt\"\n",
    "with open(test_annot_path) as file:\n",
    "    test_annots = [line.rstrip().split() for line in file.readlines()]\n",
    "test_img_path = [os.path.join(test_dir, \"0\"+test_annot[0][:3], test_annot[0]+\".jpg\") for test_annot in test_annots]\n",
    "test_int_lbl = [np.argmax(np.array([int(ast.literal_eval(x)) for x in test_annot[1:]])) for test_annot in test_annots]\n",
    "test_str_lbl = [class_map_i2s[x] for x in test_int_lbl]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
